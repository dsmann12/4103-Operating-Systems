Main Memory
~~~~~~~~~~~


Background
==========


Basic Hardware
--------------

Main memory and registers are the only general-purpose storage directly
accessible by the CPU.  Registers can be accessed in one clock cycle.  Main
memory takes several cycles because data must travel along the CPU bus.
Because of that, the CPU may need to **stall**.  The remedy is to add an
intermediary storage called a **cache** which can retrieve blocks of data from
memory, then load them into registers as needed.

Protection also needs to be enabled; we must make sure that a user process can
only access its allocated memory. For that we need to ensure each process has
its own memory space.  We do this using a **base register** and a **limit
register**.  The base register points to the first address of a memory segment,
and the limit register the last address.  These registers are checked for each
memory retrieval, and if the memory is not in bounds, then a trap is sent to
the OS.


Address Binding
---------------

Processes must be loaded into memory before they can be executed.  Process
waiting to be brought into memory are put into an **input queue**.  A compiler
typically **binds** addresses to relocatable addresses (+12).  The linkage
editor or loader binds the relocatable address to absolute addresses
(0x003E4D).

Binding can be done at different times:

  * *Compile time*. **Absolute code** can be generated if we know exactly
    where in memory it will be placed. MS-DOS .COM-format programs are bound
    at compile time.

  * *Load time*. **Relocatable code** is generated by the compiler, which 
    be bound once the program loads.  Binding is done by the loader.

  * *Execution time*. If a process is enabled to move around in memory, then
    it must be bound at execution time.  Most OSs use this method.


Logical vs. Physical Addresses
------------------------------

Addresses generated by the CPU are generally called **logical addresses**,
whereas those from the viewpoint of memory are called **physical addresses**.
Logical addresses are also called **virtual addresses**.  All logical addresses
consistute what is called a **logical address space**, whereas physical
addresses constitute a **physical address space**.  

Address bindings differ between the CPU and main memory during execution time.
To translate logical to physical addresses, a **memory management unit (MMU)**
is used. The MMU adds the base address in a **relocation register** and adds it
to the offset of the logical address to produce a physical address.  These
physical addresses can be loaded into the **memory-address register**  of the
memory to indicate its position.

Thus user programs never see real physical addresses. 


Dynamic Loading and Linking
---------------------------

**Dynamic loading** only loads routines once called.  The main program is
loaded into memory and executed at start of execution; but only when a routine
needs to be called is it loaded into memory.  This increases memory-space
utilization. 

**Dynamic linking** links system libraries to user programs at run-time.  This
enables only one copy of system libraries to exist in memory.  **Static
linking** links at load time, which means copies of system library code exist
in each process (but not in the objects).  In either case, until run-time, the
program has undefined symbols.   Dynamic linking includes a **stub** in the
image.  A stub is a piece of code which indicates how to find the library code
if it is not already loaded.

Each program must have a copy of its library available on the system to be
linked at run-time.  Sometimes updates to libraries are done; to prevent
library updates from breaking old code, the library version numbers are
maintained, and the program keeps track of the version of the library run.
Such libraries are called **shared libraries**.  You can find them in ``/lib``
and ``/usr/lib``, and generally they have ``.so`` extensions (which stands for
"shared object"). 


Swapping
========

If memory is full, the OS may **swap** processes out to a **backing store** to
make room for new ones.  Typically the backing store is a fast disk (an SSD is
ideal). This makes it possible for the physical address space of the system to
exceed the actual physical memory of the system.

The system maintains a **ready queue** of all processes whose memory images are
on the backing store or in memory and are ready to run. Whenever the scheduler
decides to execute a process, it calls the dispatcher, which checks to whether
the next process in the queue is in memory. If not, and there is no free
memory, the dispatcher swaps a process currently in memory out for the 
next ready process.

Context-switch time for swapping is high.  A 200MB process to be swapped in
from a hard disk with a 50MB transfer rate would take 4 seconds.  If we want to
swap a process, we should ensure it is idle, and in particular not waiting for
any I/O.  If we do want to swap in processes waiting for I/O, we must load the
I/O data into an OS buffer to be loaded into the process once it is swapped in.
This uses what is called **double buffering**, and requires overhead.  We must
now copy kernel-memory data into user-memory so the user process to access it.

Swapping is only used when the amount of free memory is low.


Contiguous Memory Allocation
============================

**Contiguous memory allocation** allocates each process in a single section
of memory that is contiguous to the section containing the next process. In
other words, processes are side-by-side in memory.


Memory Protection
-----------------

To protect memory, we can use the base and limit registers.  If a process
wishes to access memory, check to see if it is inside the bounds of these
registers.

This scheme allows for the OS size to change.  This flexibility may be
needed, in particular for modular kernels.  Device drivers may need to be
inserted or removed.  Such code is called **transient** OS code.


Memory Allocation
-----------------

One of the simplest ways to allocate memory is to do so in multiple fixed-size
**partitions**.  Each partition may contain one process.  In this
**multiple-partition method**, once a process is ready to run, a partition is
selected for it to be loaded into.

In a **variable-partition** scheme, the OS keeps a table indicating which parts
of memory are available and which are occupied.  Free memory segments form what
are called **holes**.  At any given time, the OS has an input queue and list of
holes.  The OS can order the processes according to a scheduling algorithm.
When a process is fit into a hole, memory is divided into two parts: the part
allocated for the process, and the remainder, which forms a smaller hole.  If
two holes are found to be adjacent, they are combined to form a larger hole.
This continues until there is no hold large enough to fit the next process, in
which case it must select another, or else wait until one becomes available.

This procedure is an instance of the **dynamic storage-allocation problem**;
there are several solutions:

  * **First fit**.  Find the first hole large enough. Fast and simple.

  * **Best fit**. Find the smallest hole which is large enough. We must
    search the entire list to find the maximin.

  * **Worst fit**. Find the largest hole. We must search the entire list
    to find the maximum.

First fit and best fit are better than worst fit in terms of decreasing time
and increasing storage utilization.  First fit is faster than best fit. 


Fragmentation
-------------

All strategies, in particular first fit and best fit, suffer from **external
fragmentation**--having small-sized non-contiguous holes which no process can
fit into.  To solve this, one can **defragment** memory or use **compaction**,
but it is expensive to move processes around in memory like this.

External fragmentation can have a drastic effect on memory utilization.
Analyses of first fit reveal that given *N* allocated blocks, another .5 *N*
blocks will be lost to fragmentation.  That is, nearly one-third of memory may
become unusable (called the **50-percent rule**).

Suppose we allocate for 0xFFFD bytes from a 0xFFFF memory hole. Then we are
left with 0x0002 bytes, but this requires more memory to keep track of than
exists in the hole.  To avoid this, we may choose to break memory into
fixed-size blocks and choose to allocate memory in blocks.  However then
memory may suffer from **internal fragmentation** from the leftover, unused
memory within a block.

One solution to fragmentation is to allow the logical address space to be
noncontiguous, thus allowing a process in physical memory to be allocated
wherever memory is available.


Segmentation
------------

**Segmentation** is a memory-management scheme that breaks memory into
purpose-driven segments: the main program, symbol table, stack, subroutines,
etc.  Then the program can refer to addresses by a segment name and an offset.
Segment names can be mapped to natural numbers.  A logical address can
therefore consist of a two-tuple, <n, m> where n is the segment number, and m
is an offset.  Normally when a program is compiled, the compiler constructs
segments.

The C compiler creates segments for code (text), globals (data), heap, stack,
and the C standard library.  Libraries linked during compile time might be
assigned their own segments. The loader assigns these segments numbers.

You can use ``readelf -a`` to examine the segments of a load module. Also
``objdump -ad`` will show a disassembly of some of the segments.


Segmentation Hardware
---------------------

We must map two-dimensional user addresses to one-dimensional physical
addresses, so we use a **segment table**, with each entry having a **segment
base** and a **segment limit**.  The base has the base address, the limit
has the size.  (Registers/caches?) 


Paging
======

**Paging** breaks physical memory into equal-sized units called **frames** and
logical memory into same-sized units called **pages**.  When a process is to be
executed, pages are loaded onto the appropriate memory frames.  This means that
a logical address space is completely separated from the physical address
space, so a process could have a 64-bit address space even though the system
has less than 2^64 bytes (16 exabytes) of physical memory. 


::

      2^0      b          2^40    TB
      2^10    kB          2^50    PB
      2^20    MB          2^60    EB
      2^30    GB          2^70    ZB


Every address generated by the CPU is divided into two parts, a **page number**
*p* and a **page offset** (d).  Any address can be represented as a two-tuple
(p, d). The page number is used as an index to a **page table**. The page table
contains the base address of each page in physical memory.  The base address
combines with the page offset to yield the physical address.

Page size is defined by hardware.  The size of a page is a power of 2.  In
modern machines, it is between 512 bytes and 1 GB.  The typical size for modern
Linux machines is 4K.  You can get page size using the command ``getconf
PAGESIZE`` or by the system call ``getpagesize()``. 

If the size of the logical address space is 2^m, and a page size is 2^n bytes,
then the m-n bits of the logical address designate the page number, and the n
low-order bits designate the page offset.  This is because (2^m / 2^n), that is
the total number of pages, is 2^(m-n).  Since the page size is 2^n itself, we
need n bytes to describe an address within a page. 

The physical memory may need to be aware of which frames are in use, so it
may maintain a **frame table** separate from the page table.


Hardware Support
----------------

Each OS has its own methods for storing page tables.  Some have a per-process
page table.  A pointer to the page table is stored with other register values
in the PCB.  When the dispatcher starts a process, it loads the user registers
and defines the correct hardware page-table values from the stored user
page table.  

Other OSs have just one page table. In the simplest case, it is implemented as
a set of dedicated registers.  The use of registers is acceptable if the page
table is small (2^8 entries).  Most contemporary computers allow the page table
to be large; for these computers, the page table is kept in main memory, and a
**page-table base register** (PTBR) points to it.  Changing page tables (as in
the case of having one page table per process) requires changing just this
register, which reduces context-switch time.

The problem with this is the time required to access a memory location.  It
requires two memory accesses; one to look up the physical address in the page
table, then the other to access that address.  The solution to this is to use a
small, fast lookup hardware cache called a **translation look-aside buffer**
(TLB).  It is associative, high-speed memory (associative means containing
key-value pairs).  The TLB stores page numbers as keys and returns frame
numbers as values.  Lookups can be performed within the instruction pipeline,
but to support that the TLB must be kept small, typically between 2^5 and 2^10
entries.  Once the lookup returns a succeeds, the frame number is combined with
the page offset to access the address.

If a page is not in the TLB, this constitutes a **TLB miss**.  In this case,
a memory reference to the page must be made.  This may be done automatically
in hardware.  If the TLB is full, an existing entry must be replaced. Some
can be made irreplaceable (**wired down**), such as pages for kernel code.

Some TLBs have **address-space identifiers** (ASIDs) in each TLB entry. ASIDs
uniquely identify processes, and check to make sure accessed pages belong to
those processes (resulting in a TLB miss if not).  This is particularly useful
in architectures for which there is a single page table.

The percentage of times the page number is found in the TLB is called the **hit
ratio**. We can use this to calculate the **effective memory-access time**.  If
it takes 50 ns to access memory, and if the hit ratio is .6, then the effective
access time is (.6)(50) + (.4)(100) = 30 + 40 = 70 ns.  That is, 60% of the time
it is a single memory access and 40% of the time it requires two--one to look
up the frame number, and another to access the desired byte in memory.  In this
way, we can also evaluate the effect of increases in hit rate or decreases in
access time.

Software-wise, ``mmap(2)`` is used to map data to memory and create a page
table entry, initializing the entry with appropriate permissions.

Protection
----------

To protect pages, we have protection bits associated with each frame, which are
kept in a page table.  A bit can define a page to be read-write or read-only. 
Invalid accesses case hardware trap to the operating system.

In Linux systems, ``mprotect(2)`` is used to protect pages.  It accepts a
page-aligned address, length and protection flags, which can be PROT_READ,
PROT_WRITE and PROT_EXEC.  ``mprotect(2)`` only works for pages that were
mapped to memory by ``mmap(2)``, since ``mmap(2)`` initializes the metadata for
its page; for other pages, its behavior is undefined. 

An additional bit generally available in the page tables is the
**valid-invalid** bit.  When set to valid, it means the page is in the
process's logical address space; if invalid it is not.  The OS may set this
bit to allow access to that page.

Since pages are contiguous, the page table length can determine if an address
is in the logical address space of the process.  Some systems therefore have a
**page-table length register** (PTLR) to indicate the size of the page table.
This value is checked against every logical address to verify that the address
is in the valid range for that process.  


Shared Pages
------------

The advantage of paging is the possibility of sharing common code, which is
particularly important in a time-sharing environment.  Consider a text editor
with 100 KB of code and 50 KB of data space used by 40 users.  To support all
of them we need 6000 KB of space; the duplicate code takes 3900 KB, or roughly
two-thirds of the total. If the code is **reentrant code** or **pure code**
(non-self-modifying) it can be shared. 

The page table maps onto the same physical copy of the editor code, but data
pages are mapped onto different frames.  Now the total memory requirement is
2100 KB.  Other heavily used programs can be shared, such as compilers, window
systems, run-time libraries, database systems, and so on.


Structure of the Page Table
===========================

Three main types of structures for page tables:

  * Hierarchical
  * Hashed page tables
  * Inverted page tables


Hierarchical Paging
-------------------

For large logical addresses spaces (2^32 or 2^64), the page table itself
becomes large.  Consider 32-bit systems with 4 KB pages (2^12). Then the page
table may consist of 2^20 entries (or 1 MB of entries). Each entry in the page
table may require up to 32 bits; thus each process may need up to 4 MB of
physical address space for the page table.  It is typical for more than 128
processes to run on a modern system; in such a case, the page table
requirements may easily balloon to an excess of 512 MB.

We can divide the page table into smaller pieces using a two-level paging
algorithm, in which the page table itself is also paged.  Consider 32-bit
system with 4 KB pages.  Then the logical address is divided into a page number
of 20 bits with a 12-bit offset.  Since we page the page table, the page number
is further divided.  We could divide the inner page number into a 10-bit outer
page number and 10-bit displacement (why 10? Because 4 bytes per entry and each
page is 2^12 in size).  This is known as a **forward-mapped page table**.

For 64-bit systems, two-level paging breaks down.  Consider 4 KB pages (2^12).
Then we have 2^52 entries with (4 PB of entries). If we use a two-level scheme,
the inner page tables can be one page long (conveniently), or have 2^10 4-byte
entries.  However the outer page table still consists of 2^42 entries (4 TB of
entries) and at 4 bytes a piece, requires 2^44 bytes (16 TB).  This is more
memory than modern systems have... so we divide again, to get 2^34 bytes (16
GB), then again to get 2^24 (16 MB).  Now we have a four-level page table,
which requires four memory accesses to get a single byte.  This is a
prohibitive number--so bad that we might as well be accessing a swap partition.


Hashed Page Tables
------------------

Instead we can use a **hashed page table**, with the hash value being the
virtual page number.  Each entry in the hash table contains a linked list of
elements that hash to the same location.  Each node in the list has three
fields: (1) the virtual page number, (2) the value of the mapped page frame,
and (3) a pointer to the next element in the list.  The virtual page number in
the virtual address is hashed into the hash table.  It is compared with field
(1) in each element element in the linked list until found.  

A variation useful for 64-bit addresses uses **clustered page tables**, similar
to hashed page tables except each entry in the hash table refers to several
pages. A single page-table entry can store mappings for multiple physical-page
frames. Clustered page tables are useful for **sparse** address spaces. 


Inverted Page Tables
--------------------

In a normal page table, each page table may consist of millions of entries, and
thus may consume large amounts of physical memory.  To resolve this, we can use
an **inverted page table**.  An inverted page table has one entry for each real
page (frame) of memory. Each entry consists of the virtual address of the page
stored in that real memory location, with information about the process that
owns the page.  Inverted page tables often require ASIDs.

Inverted page-table entries have <process-id, page-number> tuples. When a
memory reference occurs (consisting of such a <process-id, page-number> tuple>,
the page table is searched. Suppose the i^th entry is found. Then the physical
address <i, offset> is generated.

This decreases memory required because only the existing pages are stored; but
it increases the amount of time required since the table has to be searched.

